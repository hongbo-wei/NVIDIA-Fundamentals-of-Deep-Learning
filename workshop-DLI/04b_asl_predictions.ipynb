{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EKFbMeI4Csut"
   },
   "source": [
    "<center><a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GyRFI0IJCsuu"
   },
   "source": [
    "# 4b. Deploying Your Model\n",
    "Now that we have a well trained model, it's time to use it. In this exercise, we'll expose new images to our model and detect the correct letters of the sign language alphabet. Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wdYYMU1WCsuv"
   },
   "source": [
    "## 4b.1 Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SeXBHUIjCsuv"
   },
   "source": [
    "* Load an already-trained model from disk\n",
    "* Reformat images for a model trained on images of a different format\n",
    "* Perform inference with new images, never seen by the trained model and evaluate its performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 260,
     "status": "ok",
     "timestamp": 1715244405026,
     "user": {
      "displayName": "Danielle Detering US",
      "userId": "15432464718872067879"
     },
     "user_tz": 420
    },
    "id": "idwYaoghaqIp",
    "outputId": "79b63437-52ca-448c-92c5-59857227be18"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.io as tv_io\n",
    "import torchvision.transforms.v2 as transforms\n",
    "import torchvision.transforms.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xo-pr5-zCsuv"
   },
   "source": [
    "## 4b.2 Loading the Model\n",
    "Now that we're in a new notebook, let's load the saved model that we trained. Our save from the previous exercise created a folder called \"asl_model\". We can load the model by selecting the same folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our model uses a [custom module](https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html), we will need to load the code for that class. We have saved a copy of the code in [utils.py](./uitls.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 169,
     "status": "ok",
     "timestamp": 1715242471134,
     "user": {
      "displayName": "Danielle Detering US",
      "userId": "15432464718872067879"
     },
     "user_tz": 420
    },
    "id": "jiI87fNVZdRl"
   },
   "outputs": [],
   "source": [
    "from utils import MyConvBlock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a definition for `MyConvBlock`, we can use [torch.load](https://pytorch.org/docs/stable/generated/torch.load.html) to load a model from a path. We can use `map_location to specify the device. When we print the model, does it look the same as in the last notebook?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 180,
     "status": "ok",
     "timestamp": 1715242473387,
     "user": {
      "displayName": "Danielle Detering US",
      "userId": "15432464718872067879"
     },
     "user_tz": 420
    },
    "id": "-Xy1JuL-Csuv"
   },
   "outputs": [],
   "source": [
    "model = torch.load('model.pth', map_location=device)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WIjqbFCFCsuv"
   },
   "source": [
    "We can also verify if the model is on our GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 174,
     "status": "ok",
     "timestamp": 1715242474520,
     "user": {
      "displayName": "Danielle Detering US",
      "userId": "15432464718872067879"
     },
     "user_tz": 420
    },
    "id": "_ulCst9GCsuw",
    "outputId": "0d55b14d-9b52-49e7-efda-76a05abe21ed"
   },
   "outputs": [],
   "source": [
    "next(model.parameters()).device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pBBneFIXCsuw"
   },
   "source": [
    "## 4b.3 Preparing an Image for the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gffCLeacCsuw"
   },
   "source": [
    "It's now time to use the model to make predictions on new images that it's never seen before. This is also called inference. We have a set of images in the `data/asl_images` folder. Try opening it using the left navigation and explore the images.\n",
    "\n",
    "You'll notice that the images we have are much higher resolution than the images in our dataset. They are also in color. Remember that our images in the dataset were 28x28 pixels and grayscale. It's important to keep in mind that whenever we make predictions with a model, the input must match the shape of the data that the model was trained on. For this model, the training dataset was of the shape: (27455, 28, 28, 1). This corresponded to 27455 images of 28 by 28 pixels each with one color channel (grayscale)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UXeujDRFCsuw"
   },
   "source": [
    "### 4b.3.1 Showing the Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8gLvKQ1yCsuw"
   },
   "source": [
    "When we use our model to make predictions on new images, it will be useful to show the image as well. We can use the matplotlib library to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 131,
     "status": "ok",
     "timestamp": 1715243759243,
     "user": {
      "displayName": "Danielle Detering US",
      "userId": "15432464718872067879"
     },
     "user_tz": 420
    },
    "id": "qzzai3QZCsuw"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "def show_image(image_path):\n",
    "    image = mpimg.imread(image_path)\n",
    "    plt.imshow(image, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 434
    },
    "executionInfo": {
     "elapsed": 745,
     "status": "ok",
     "timestamp": 1715243543244,
     "user": {
      "displayName": "Danielle Detering US",
      "userId": "15432464718872067879"
     },
     "user_tz": 420
    },
    "id": "ZFzwvkalCsuw",
    "outputId": "74ad4eac-4f99-413f-9d9f-54cad0d5d7e7"
   },
   "outputs": [],
   "source": [
    "show_image('data/asl_images/b.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ORPUII6qCsux"
   },
   "source": [
    "### 4b.3.2 Scaling the Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eYcawjxlCsux"
   },
   "source": [
    "The images in our dataset were 28x28 pixels and grayscale. We need to make sure to pass the same size and grayscale images into our method for prediction. There are a few ways to edit images with Python, but TorchVision also has the [read_image](https://pytorch.org/vision/stable/generated/torchvision.io.read_image.html) function. We can let it know what kind of image to read with [ImageReadMode](https://pytorch.org/vision/stable/generated/torchvision.io.ImageReadMode.html#torchvision.io.ImageReadMode)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 158,
     "status": "ok",
     "timestamp": 1715244417167,
     "user": {
      "displayName": "Danielle Detering US",
      "userId": "15432464718872067879"
     },
     "user_tz": 420
    },
    "id": "lDJu6JrEffRP",
    "outputId": "927907d8-3c33-4a3c-bd9f-3b1844a40b2d"
   },
   "outputs": [],
   "source": [
    "image = tv_io.read_image('data/asl_images/b.png', tv_io.ImageReadMode.GRAY)\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the shape of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1715244124709,
     "user": {
      "displayName": "Danielle Detering US",
      "userId": "15432464718872067879"
     },
     "user_tz": 420
    },
    "id": "8iJ4A8YagRvP",
    "outputId": "858dc8bc-8595-4189-8738-954ac6630d0c"
   },
   "outputs": [],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This image is much larger than what we trained on. We can use [TorchVision](https://pytorch.org/vision/stable/index.html)'s [Transforms](https://pytorch.org/vision/0.9/transforms.html) again to get the data in the form our model expects.\n",
    "\n",
    "We will:\n",
    "* Convert the image to float with [ToDtype](https://pytorch.org/vision/stable/generated/torchvision.transforms.v2.ToDtype.html)\n",
    "  * We will set `scale` to `True` in order to convert from [0, 255] to [0, 1]\n",
    "* [Resize](https://pytorch.org/vision/stable/generated/torchvision.transforms.v2.Resize.html#torchvision.transforms.v2.Resize) the image to be 28 x 28 pixels\n",
    "* Convert the images to [Grayscale](https://pytorch.org/vision/stable/generated/torchvision.transforms.v2.Grayscale.html#torchvision.transforms.v2.Grayscale)\n",
    "  * This step doesn't do anything since our models are already grayscale, but we've added it here to show an alternative way to get grayscale images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 337,
     "status": "ok",
     "timestamp": 1715244596376,
     "user": {
      "displayName": "Danielle Detering US",
      "userId": "15432464718872067879"
     },
     "user_tz": 420
    },
    "id": "lNp6Q9cBe8Xw"
   },
   "outputs": [],
   "source": [
    "IMG_WIDTH = 28\n",
    "IMG_HEIGHT = 28\n",
    "\n",
    "preprocess_trans = transforms.Compose([\n",
    "    transforms.ToDtype(torch.float32, scale=True), # Converts [0, 255] to [0, 1]\n",
    "    transforms.Resize((IMG_WIDTH, IMG_HEIGHT)),\n",
    "    transforms.Grayscale()  # From Color to Gray\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test `preprocess_trans` on an image to make sure it works correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 164,
     "status": "ok",
     "timestamp": 1715244649391,
     "user": {
      "displayName": "Danielle Detering US",
      "userId": "15432464718872067879"
     },
     "user_tz": 420
    },
    "id": "BBS7SjnBhIe6",
    "outputId": "fadd047d-3763-4b05-ea2e-ebe0427bf30d"
   },
   "outputs": [],
   "source": [
    "processed_image = preprocess_trans(image)\n",
    "processed_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numbers look correct, but how about the shape?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's plot the image to see if it looks like what we trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_image = F.to_pil_image(processed_image)\n",
    "plt.imshow(plot_image, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking good! Let's pass it to our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4b.4 Making Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vYl2JcUJCsuy"
   },
   "source": [
    "Okay, now we're ready to predict! Our model still expects a batch of images. If the [squeeze](https://pytorch.org/docs/stable/generated/torch.squeeze.htmlhttps://pytorch.org/docs/stable/generated/torch.squeeze.html) removes dimensions of 1, [unsqueeze](https://pytorch.org/docs/stable/generated/torch.unsqueeze.htmlhttps://pytorch.org/docs/stable/generated/torch.unsqueeze.html) adds a dimension of 1 at the index we specify. The first dimension is usually the batch dimension, so we can say `.unsqueeze(0)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 168,
     "status": "ok",
     "timestamp": 1715244907170,
     "user": {
      "displayName": "Danielle Detering US",
      "userId": "15432464718872067879"
     },
     "user_tz": 420
    },
    "id": "L-xZGe1SCsux",
    "outputId": "12b26af6-e1ce-4989-864b-9e26419c4f84"
   },
   "outputs": [],
   "source": [
    "batched_image = processed_image.unsqueeze(0)\n",
    "batched_image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we should make sure the input tensor is on the same `device` as the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_image_gpu = batched_image.to(device)\n",
    "batched_image_gpu.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to pass it to the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 176,
     "status": "ok",
     "timestamp": 1715245336961,
     "user": {
      "displayName": "Danielle Detering US",
      "userId": "15432464718872067879"
     },
     "user_tz": 420
    },
    "id": "WhLIaQIhkE0c",
    "outputId": "f3fe15e3-f524-4218-c278-31e740299237"
   },
   "outputs": [],
   "source": [
    "output = model(batched_image_gpu)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CCleBaVCCsuz"
   },
   "source": [
    "### 4b.4.1 Understanding the Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QNIv09wxCsuz"
   },
   "source": [
    "The predictions are in the format of a 24 length array. The larger the value, the more likely the input image belongs to the corresponding class. Let's make it a little more readable. We can start by finding which element of the array represents the highest probability. This can be done easily with the numpy library and the [argmax](https://numpy.org/doc/stable/reference/generated/numpy.argmax.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 171,
     "status": "ok",
     "timestamp": 1715245408487,
     "user": {
      "displayName": "Danielle Detering US",
      "userId": "15432464718872067879"
     },
     "user_tz": 420
    },
    "id": "gm6J4jxcCsuz"
   },
   "outputs": [],
   "source": [
    "prediction = output.argmax(dim=1).item()\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BpZzxObRCsuz"
   },
   "source": [
    "Each element of the prediction array represents a possible letter in the sign language alphabet. Remember that j and z are not options because they involve moving the hand, and we're only dealing with still photos. Let's create a mapping between the index of the predictions array, and the corresponding letter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 139,
     "status": "ok",
     "timestamp": 1715245412136,
     "user": {
      "displayName": "Danielle Detering US",
      "userId": "15432464718872067879"
     },
     "user_tz": 420
    },
    "id": "1ExBb6fMCsuz"
   },
   "outputs": [],
   "source": [
    "# Alphabet does not contain j or z because they require movement\n",
    "alphabet = \"abcdefghiklmnopqrstuvwxy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uXy2Ze9kCsuz"
   },
   "source": [
    "We can now pass in our prediction index to find the corresponding letter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "elapsed": 140,
     "status": "ok",
     "timestamp": 1715245413856,
     "user": {
      "displayName": "Danielle Detering US",
      "userId": "15432464718872067879"
     },
     "user_tz": 420
    },
    "id": "VNxyHNZmCsuz",
    "outputId": "87063db2-9c3d-45ed-f32f-6e5700fde9c8"
   },
   "outputs": [],
   "source": [
    "alphabet[prediction]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hs2_6guPCsuz"
   },
   "source": [
    "#### Exercise: Put it all Together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KobsgLirCsuz"
   },
   "source": [
    "Let's put everything in a function so that we can make predictions just from the image file. Implement it in the function below using the functions and steps above. If you need help, you can reveal the solution by clicking the three dots below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f3prZEMgCsuz"
   },
   "outputs": [],
   "source": [
    "def predict_letter(file_path):\n",
    "    # Show image\n",
    "    FIXME\n",
    "    # Load and grayscale image\n",
    "    image = FIXME\n",
    "    # Transform image\n",
    "    image = FIXME\n",
    "    # Batch image\n",
    "    image = FIXME\n",
    "    # Send image to correct device\n",
    "    image = FIXME\n",
    "    # Make prediction\n",
    "    output = FIXME\n",
    "    # Find max index\n",
    "    prediction = FIXME\n",
    "    # Convert prediction to letter\n",
    "    predicted_letter = FIXME\n",
    "    # Return prediction\n",
    "    return predicted_letter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "enmPt4goCsu0"
   },
   "source": [
    "#### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zfp8Cxm9Csu0"
   },
   "source": [
    "Click on the '...' below to view the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 146,
     "status": "ok",
     "timestamp": 1715245521266,
     "user": {
      "displayName": "Danielle Detering US",
      "userId": "15432464718872067879"
     },
     "user_tz": 420
    },
    "id": "wTUx5fWMCsu_",
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "def predict_letter(file_path):\n",
    "    show_image(file_path)\n",
    "    image = tv_io.read_image(file_path, tv_io.ImageReadMode.GRAY)\n",
    "    image = preprocess_trans(image)\n",
    "    image = image.unsqueeze(0)\n",
    "    image = image.to(device)\n",
    "    output = model(image)\n",
    "    prediction = output.argmax(dim=1).item()\n",
    "    # convert prediction to letter\n",
    "    predicted_letter = alphabet[prediction]\n",
    "    return predicted_letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 453
    },
    "executionInfo": {
     "elapsed": 648,
     "status": "ok",
     "timestamp": 1715245523822,
     "user": {
      "displayName": "Danielle Detering US",
      "userId": "15432464718872067879"
     },
     "user_tz": 420
    },
    "id": "MWqiMAhwCsu_",
    "outputId": "32476dc7-79ef-4280-ba34-725a46837aae"
   },
   "outputs": [],
   "source": [
    "predict_letter(\"data/asl_images/b.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JNZSUoinCsvA"
   },
   "source": [
    "Let's also use the function with the 'a' letter in the asl_images datset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 453
    },
    "executionInfo": {
     "elapsed": 754,
     "status": "ok",
     "timestamp": 1715245527515,
     "user": {
      "displayName": "Danielle Detering US",
      "userId": "15432464718872067879"
     },
     "user_tz": 420
    },
    "id": "xk0F5w53CsvA",
    "outputId": "1cfe6713-59ec-4ce4-d4de-22d4cf464783"
   },
   "outputs": [],
   "source": [
    "predict_letter(\"data/asl_images/a.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cFaxufFbCsvA"
   },
   "source": [
    "## 4b.5 Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZtsAm4TJCsvA"
   },
   "source": [
    "Great work on these exercises! You've gone through the full process of training a highly accurate model from scratch, and then using the model to make new and valuable predictions. If you have some time, we encourage you to take pictures with your webcam, upload them by dropping them into the data/asl_images folder, and test out the model on them. For Mac you can use Photo Booth. For windows you can select the Camera app from your start screen. We hope you try it. It's a good opportunity to learn some sign language! For instance, try out the letters of your name.\n",
    "\n",
    "We can imagine how this model could be used in an application to teach someone sign language, or even help someone who cannot speak interact with a computer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zyfxj0v0CsvA"
   },
   "source": [
    "### 4b.5.1 Clear the Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JtO6ds0LCsvA"
   },
   "source": [
    "Before moving on, please execute the following cell to clear up the GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mN85KA47CsvA"
   },
   "outputs": [],
   "source": [
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jKNrUtH1CsvA"
   },
   "source": [
    "### 4b.5.2 Next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k1VRSuYLCsvA"
   },
   "source": [
    "We hope you've enjoyed these exercises. In the next sections we will learn how to take advantage of deep learning when we don't have a robust dataset available. See you there!\n",
    "To learn more about inference on the edge, check out [this paper](http://web.eecs.umich.edu/~mosharaf/Readings/FB-ML-Edge.pdf) on the topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QnOZX0ClCsvA"
   },
   "source": [
    "Now that we're familiar building your own models and have some understanding of how they work, we will turn our attention to the very powerful technique of using pre-trained models to expedite your work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4bnILHG8CsvA"
   },
   "source": [
    "<center><a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a></center>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
